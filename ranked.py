# -*- coding: utf-8 -*-
"""ranked.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13p1m4W6PQk5-6ACvMu0d4AJ_makqjCcT
"""

import requests
import pandas as pd
from datetime import datetime
from google.colab import files
import time
import io
import os
import uuid

try:
    from hashids import Hashids
    HASHIDS_AVAILABLE = True
except ImportError:
    HASHIDS_AVAILABLE = False
    print("⚠️  Hashids not installed. Run: pip install hashids")

REAL_API_BASE = "https://web.realsports.io"
REAL_VERSION = "27"
RANKED_DAYS_API = f"{REAL_API_BASE}/rankeddays"

REAL_AUTH_TOKEN = os.environ.get("REAL_AUTH_TOKEN")
if not REAL_AUTH_TOKEN:
    try:
        from getpass import getpass
        REAL_AUTH_TOKEN = getpass("Enter RealSports auth token: ")
    except Exception:
        REAL_AUTH_TOKEN = None

DEVICE_UUID = os.environ.get("REAL_DEVICE_UUID") or str(uuid.uuid4())
DEFAULT_USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36"


def generate_request_token() -> str:
    if not HASHIDS_AVAILABLE:
        raise RuntimeError("hashids is required. Install with: pip install hashids")
    hashids = Hashids(salt="realwebapp", min_length=16)
    return hashids.encode(int(time.time() * 1000))


def build_real_headers(device_name: str = "Chrome on Windows") -> dict:
    if not REAL_AUTH_TOKEN:
        raise RuntimeError("REAL_AUTH_TOKEN is not set.")
    return {
        "Accept": "application/json",
        "Content-Type": "application/json",
        "DNT": "1",
        "Origin": "https://realsports.io",
        "Referer": "https://realsports.io/",
        "User-Agent": DEFAULT_USER_AGENT,
        "real-auth-info": REAL_AUTH_TOKEN,
        "real-device-name": device_name,
        "real-device-type": "desktop_web",
        "real-device-uuid": DEVICE_UUID,
        "real-request-token": generate_request_token(),
        "real-version": REAL_VERSION,
    }

def scrape_ranked_days(username, user_id):
    """
    Scrape all ranked days data for a given user ID.

    Args:
        username (str): The username for identification
        user_id (str): The alphanumeric user ID

    Returns:
        list: List of dictionaries containing username, userId, day, karma, and rank data
    """
    base_url = RANKED_DAYS_API
    all_data = []
    oldest_date = None

    print(f"Scraping data for user: {username} (ID: {user_id})")

    while True:
        # Construct URL
        if oldest_date is None:
            url = f"{base_url}/{user_id}?sort=latest"
        else:
            url = f"{base_url}/{user_id}?before={oldest_date}&sort=latest"

        print(f"  Fetching: {url}")

        try:
            # Make request
            response = requests.get(url, headers=build_real_headers())
            response.raise_for_status()  # Raise exception for bad status codes

            # Parse JSON
            data = response.json()

            # Check if days array is empty
            if not data.get('days') or len(data['days']) == 0:
                print(f"  No more data found. Stopping.")
                break

            # Extract day, karma, and rank for each entry
            for entry in data['days']:
                all_data.append({
                    'username': username,
                    'userId': user_id,
                    'day': entry.get('day'),
                    'karma': entry.get('karma'),
                    'rank': entry.get('rank')
                })

            # Update oldest_date for next iteration
            oldest_date = data['days'][-1]['day']
            print(f"  Collected {len(data['days'])} entries. Oldest date: {oldest_date}")

            # Small delay to be respectful to the API
            time.sleep(0.5)

        except requests.exceptions.RequestException as e:
            print(f"  Error fetching data: {e}")
            break
        except (KeyError, ValueError) as e:
            print(f"  Error parsing response: {e}")
            break

    print(f"Total entries collected for {username}: {len(all_data)}\n")
    return all_data

def main():
    """
    Main function to scrape data for multiple users and save to CSV.
    """
    # Upload CSV file
    print("Please upload your CSV file with columns: username,userId")
    uploaded = files.upload()

    if not uploaded:
        print("No file uploaded!")
        return

    # Get the first (and should be only) uploaded file
    filename = list(uploaded.keys())[0]

    # Read the CSV
    try:
        users_df = pd.read_csv(io.BytesIO(uploaded[filename]))

        # Verify required columns exist
        if 'username' not in users_df.columns or 'userId' not in users_df.columns:
            print("Error: CSV must contain 'username' and 'userId' columns!")
            return

        print(f"\nLoaded {len(users_df)} user(s) from {filename}")
        print(users_df.head())
        print(f"\nProcessing {len(users_df)} user(s)...\n")

    except Exception as e:
        print(f"Error reading CSV file: {e}")
        return

    # Collect all data
    all_data = []
    for _, row in users_df.iterrows():
        username = row['username']
        user_id = row['userId']
        user_data = scrape_ranked_days(username, user_id)
        all_data.extend(user_data)

    # Convert to DataFrame
    if all_data:
        df = pd.DataFrame(all_data)

        # Ensure column order: username, userId, day, karma, rank
        df = df[['username', 'userId', 'day', 'karma', 'rank']]

        # Generate filename with timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_filename = f"ranked_days_{timestamp}.csv"

        # Save to CSV
        df.to_csv(output_filename, index=False)
        print(f"\nData saved to {output_filename}")
        print(f"Total records: {len(df)}")
        print(f"Records per user:")
        print(df.groupby('username').size())
        print("\nFirst few rows:")
        print(df.head(10))

        # Auto-download the file
        files.download(output_filename)
        print(f"\n{output_filename} has been downloaded!")
    else:
        print("\nNo data collected!")

# Run the script
if __name__ == "__main__":
    main()
