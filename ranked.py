# -*- coding: utf-8 -*-
"""ranked.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13p1m4W6PQk5-6ACvMu0d4AJ_makqjCcT
"""

import requests
import pandas as pd
from datetime import datetime
from google.colab import files
import time
import io

def scrape_ranked_days(username, user_id):
    """
    Scrape all ranked days data for a given user ID.

    Args:
        username (str): The username for identification
        user_id (str): The alphanumeric user ID

    Returns:
        list: List of dictionaries containing username, userId, day, karma, and rank data
    """
    base_url = "https://api.real.vg/rankeddays"
    all_data = []
    oldest_date = None

    print(f"Scraping data for user: {username} (ID: {user_id})")

    while True:
        # Construct URL
        if oldest_date is None:
            url = f"{base_url}/{user_id}?sort=latest"
        else:
            url = f"{base_url}/{user_id}?before={oldest_date}&sort=latest"

        print(f"  Fetching: {url}")

        try:
            # Make request
            response = requests.get(url)
            response.raise_for_status()  # Raise exception for bad status codes

            # Parse JSON
            data = response.json()

            # Check if days array is empty
            if not data.get('days') or len(data['days']) == 0:
                print(f"  No more data found. Stopping.")
                break

            # Extract day, karma, and rank for each entry
            for entry in data['days']:
                all_data.append({
                    'username': username,
                    'userId': user_id,
                    'day': entry.get('day'),
                    'karma': entry.get('karma'),
                    'rank': entry.get('rank')
                })

            # Update oldest_date for next iteration
            oldest_date = data['days'][-1]['day']
            print(f"  Collected {len(data['days'])} entries. Oldest date: {oldest_date}")

            # Small delay to be respectful to the API
            time.sleep(0.5)

        except requests.exceptions.RequestException as e:
            print(f"  Error fetching data: {e}")
            break
        except (KeyError, ValueError) as e:
            print(f"  Error parsing response: {e}")
            break

    print(f"Total entries collected for {username}: {len(all_data)}\n")
    return all_data

def main():
    """
    Main function to scrape data for multiple users and save to CSV.
    """
    # Upload CSV file
    print("Please upload your CSV file with columns: username,userId")
    uploaded = files.upload()

    if not uploaded:
        print("No file uploaded!")
        return

    # Get the first (and should be only) uploaded file
    filename = list(uploaded.keys())[0]

    # Read the CSV
    try:
        users_df = pd.read_csv(io.BytesIO(uploaded[filename]))

        # Verify required columns exist
        if 'username' not in users_df.columns or 'userId' not in users_df.columns:
            print("Error: CSV must contain 'username' and 'userId' columns!")
            return

        print(f"\nLoaded {len(users_df)} user(s) from {filename}")
        print(users_df.head())
        print(f"\nProcessing {len(users_df)} user(s)...\n")

    except Exception as e:
        print(f"Error reading CSV file: {e}")
        return

    # Collect all data
    all_data = []
    for _, row in users_df.iterrows():
        username = row['username']
        user_id = row['userId']
        user_data = scrape_ranked_days(username, user_id)
        all_data.extend(user_data)

    # Convert to DataFrame
    if all_data:
        df = pd.DataFrame(all_data)

        # Ensure column order: username, userId, day, karma, rank
        df = df[['username', 'userId', 'day', 'karma', 'rank']]

        # Generate filename with timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_filename = f"ranked_days_{timestamp}.csv"

        # Save to CSV
        df.to_csv(output_filename, index=False)
        print(f"\nData saved to {output_filename}")
        print(f"Total records: {len(df)}")
        print(f"Records per user:")
        print(df.groupby('username').size())
        print("\nFirst few rows:")
        print(df.head(10))

        # Auto-download the file
        files.download(output_filename)
        print(f"\n{output_filename} has been downloaded!")
    else:
        print("\nNo data collected!")

# Run the script
if __name__ == "__main__":
    main()