# -*- coding: utf-8 -*-
"""KHScrape.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M2aN6XbGXYxJCLOyEAZk4FklU6ozT4s8
"""

!pip install supabase requests hashids

from getpass import getpass
SUPABASE_URL = getpass("Enter your Supabase URL: ")
SUPABASE_KEY = getpass("Enter your Supabase API Key: ")

import os
import time
import uuid
import requests
import random
from datetime import datetime, timedelta
from typing import Optional
from dataclasses import dataclass
from hashids import Hashids

# Scraper settings
ENTRIES_PER_PAGE = 20
MAX_ENTRIES_PER_DAY = 1020
REAL_API_BASE = "https://web.realsports.io"
REAL_VERSION = "27"
BASE_URL = f"{REAL_API_BASE}/userkarmaranks/day"

REAL_AUTH_TOKEN = os.environ.get("REAL_AUTH_TOKEN")
if not REAL_AUTH_TOKEN:
    REAL_AUTH_TOKEN = getpass("Enter RealSports auth token: ")

DEVICE_UUID = os.environ.get("REAL_DEVICE_UUID") or str(uuid.uuid4())
DEFAULT_USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36"


def generate_request_token() -> str:
    hashids = Hashids(salt="realwebapp", min_length=16)
    return hashids.encode(int(time.time() * 1000))


def build_real_headers(device_name: str = "Chrome on Windows") -> dict:
    if not REAL_AUTH_TOKEN:
        raise RuntimeError("REAL_AUTH_TOKEN is not set.")
    return {
        "Accept": "application/json",
        "Content-Type": "application/json",
        "DNT": "1",
        "Origin": "https://realsports.io",
        "Referer": "https://realsports.io/",
        "User-Agent": DEFAULT_USER_AGENT,
        "real-auth-info": REAL_AUTH_TOKEN,
        "real-device-name": device_name,
        "real-device-type": "desktop_web",
        "real-device-uuid": DEVICE_UUID,
        "real-request-token": generate_request_token(),
        "real-version": REAL_VERSION,
    }

# Politeness settings
REQUEST_DELAY_SECONDS = random.uniform(0.3, 0.7)  # Delay between requests
MAX_CONSECUTIVE_ERRORS = 3   # Stop after this many consecutive errors
MAX_CONSECUTIVE_EMPTY = 2    # Stop after this many consecutive empty responses
TIMEOUT_SECONDS = 30         # Request timeout


@dataclass
class CircuitBreaker:
    """Tracks errors and empty responses to prevent hammering the API."""
    consecutive_errors: int = 0
    consecutive_empty: int = 0
    total_errors: int = 0
    is_open: bool = False

    def record_success(self):
        """Reset error counters on successful response with data."""
        self.consecutive_errors = 0
        self.consecutive_empty = 0

    def record_empty(self):
        """Record an empty response."""
        self.consecutive_empty += 1
        if self.consecutive_empty >= MAX_CONSECUTIVE_EMPTY:
            print(f"âš ï¸  Circuit breaker: {MAX_CONSECUTIVE_EMPTY} consecutive empty responses")
            return True  # Signal to stop for this day
        return False

    def record_error(self, error_msg: str):
        """Record an error and check if circuit should open."""
        self.consecutive_errors += 1
        self.total_errors += 1
        print(f"âŒ Error ({self.consecutive_errors}/{MAX_CONSECUTIVE_ERRORS}): {error_msg}")

        if self.consecutive_errors >= MAX_CONSECUTIVE_ERRORS:
            self.is_open = True
            print(f"ðŸ›‘ CIRCUIT BREAKER OPEN: {MAX_CONSECUTIVE_ERRORS} consecutive errors. Stopping all requests.")
            return True
        return False


def fetch_leaderboard_page(date_str: str, before: Optional[int], circuit: CircuitBreaker) -> list:
    """
    Fetch a single page of leaderboard data.

    Args:
        date_str: Date in YYYY-MM-DD format
        before: Offset for pagination (None for first page, 20, 40, 60, etc.)
        circuit: CircuitBreaker instance for error tracking

    Returns:
        List of user entries or empty list on failure
    """
    if circuit.is_open:
        return []

    # Build URL
    url = f"{BASE_URL}?day={date_str}"
    if before is not None and before > 0:
        url += f"&before={before}"

    try:
        response = requests.get(url, headers=build_real_headers(), timeout=TIMEOUT_SECONDS)

        if response.status_code != 200:
            circuit.record_error(f"HTTP {response.status_code} for {url}")
            return []

        data = response.json()
        users = data.get("users", [])

        if not users:
            # Empty response - might be end of data
            should_stop = circuit.record_empty()
            if should_stop:
                return []
        else:
            circuit.record_success()

        # Extract relevant fields
        entries = []
        for user in users:
            entries.append({
                "user_id": user.get("userId"),
                "username": user.get("userName"),
                "amount": user.get("amount"),
                "rank": user.get("rank"),
            })

        return entries

    except requests.exceptions.Timeout:
        circuit.record_error(f"Timeout for {url}")
        return []
    except requests.exceptions.RequestException as e:
        circuit.record_error(f"Request failed: {str(e)}")
        return []
    except ValueError as e:
        circuit.record_error(f"JSON decode error: {str(e)}")
        return []


def scrape_day(date_str: str, circuit: CircuitBreaker) -> list:
    """
    Scrape all leaderboard entries for a single day.

    Args:
        date_str: Date in YYYY-MM-DD format
        circuit: CircuitBreaker instance

    Returns:
        List of all entries for the day
    """
    if circuit.is_open:
        return []

    all_entries = []
    offset = 0

    print(f"\nðŸ“… Scraping {date_str}...")

    while offset < MAX_ENTRIES_PER_DAY and not circuit.is_open:
        # Determine the 'before' parameter
        before = offset if offset > 0 else None

        # Fetch page
        entries = fetch_leaderboard_page(date_str, before, circuit)

        if not entries:
            # No more data or circuit opened
            break

        all_entries.extend(entries)
        print(f"  âœ“ Fetched entries {offset + 1}-{offset + len(entries)} (ranks {entries[0]['rank']}-{entries[-1]['rank']})")

        # Check if we got fewer than expected (end of data)
        if len(entries) < ENTRIES_PER_PAGE:
            print(f"  â„¹ï¸  Received {len(entries)} entries (less than {ENTRIES_PER_PAGE}), likely end of data")
            break

        offset += ENTRIES_PER_PAGE

        # Polite delay
        time.sleep(REQUEST_DELAY_SECONDS)

    print(f"  ðŸ“Š Total entries for {date_str}: {len(all_entries)}")
    return all_entries


def save_to_supabase(entries: list, date_str: str, supabase_client) -> bool:
    """
    Save entries to Supabase database.

    Args:
        entries: List of entry dictionaries
        date_str: The date these entries are for
        supabase_client: Initialized Supabase client

    Returns:
        True if successful, False otherwise
    """
    if not entries:
        print("  âš ï¸  No entries to save")
        return True

    try:
        # Prepare data for insertion
        records = []
        for entry in entries:
            records.append({
                "scrape_date": date_str,
                "user_id": entry["user_id"],
                "username": entry["username"],
                "amount": entry["amount"],
                "rank": entry["rank"],
            })

        # Upsert (insert or update on conflict)
        # Using batches to avoid hitting size limits
        batch_size = 500
        for i in range(0, len(records), batch_size):
            batch = records[i:i + batch_size]
            supabase_client.table("karma_rankings").upsert(
                batch,
                on_conflict="scrape_date,user_id"
            ).execute()
            print(f"  ðŸ’¾ Saved batch {i // batch_size + 1} ({len(batch)} records)")

        print(f"  âœ… Successfully saved {len(records)} entries to Supabase")
        return True

    except Exception as e:
        print(f"  âŒ Database error: {str(e)}")
        return False


def generate_date_range(start_date: str, end_date: str) -> list:
    """
    Generate a list of dates between start and end (inclusive).

    Args:
        start_date: Start date in YYYY-MM-DD format
        end_date: End date in YYYY-MM-DD format

    Returns:
        List of date strings
    """
    start = datetime.strptime(start_date, "%Y-%m-%d")
    end = datetime.strptime(end_date, "%Y-%m-%d")

    dates = []
    current = start
    while current <= end:
        dates.append(current.strftime("%Y-%m-%d"))
        current += timedelta(days=1)

    return dates


def run_scraper(start_date: str, end_date: str = None):
    """
    Main function to run the scraper.

    Args:
        start_date: Start date in YYYY-MM-DD format
        end_date: End date in YYYY-MM-DD format (defaults to start_date if not provided)
    """
    # Initialize Supabase client
    try:
        from supabase import create_client, Client

        if SUPABASE_URL == "YOUR_SUPABASE_URL" or SUPABASE_KEY == "YOUR_SUPABASE_ANON_KEY":
            print("âŒ Please set your SUPABASE_URL and SUPABASE_KEY before running!")
            print("   Edit the configuration section at the top of this script.")
            return

        supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
        print("âœ… Connected to Supabase")
    except ImportError:
        print("âŒ Supabase library not installed. Run: !pip install supabase")
        return
    except Exception as e:
        print(f"âŒ Failed to connect to Supabase: {str(e)}")
        return

    # Handle date range
    if end_date is None:
        end_date = start_date

    dates = generate_date_range(start_date, end_date)
    print(f"\nðŸ—“ï¸  Will scrape {len(dates)} day(s): {dates[0]} to {dates[-1]}")

    # Initialize circuit breaker
    circuit = CircuitBreaker()

    # Track statistics
    total_entries = 0
    successful_days = 0

    # Scrape each day
    for date_str in dates:
        if circuit.is_open:
            print(f"\nðŸ›‘ Circuit breaker is open. Skipping remaining dates.")
            break

        # Scrape the day
        entries = scrape_day(date_str, circuit)

        if entries:
            # Save to database
            if save_to_supabase(entries, date_str, supabase):
                total_entries += len(entries)
                successful_days += 1

        # Reset empty counter between days (empty at end of day is expected)
        circuit.consecutive_empty = 0

        # Extra delay between days
        if date_str != dates[-1]:
            print(f"  â³ Waiting before next day...")
            time.sleep(REQUEST_DELAY_SECONDS * 2)

    # Summary
    print("\n" + "=" * 50)
    print("ðŸ“ˆ SCRAPING COMPLETE")
    print("=" * 50)
    print(f"  Days processed: {successful_days}/{len(dates)}")
    print(f"  Total entries saved: {total_entries}")
    print(f"  Total errors encountered: {circuit.total_errors}")
    if circuit.is_open:
        print("  âš ï¸  Scraping was interrupted by circuit breaker")

run_scraper("2025-03-07","2025-05-14")
